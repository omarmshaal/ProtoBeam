{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PotoBeam ( Prototypical Networks for Beam Classification)\n",
        "\n",
        "This project is based on the tutorial notebook available at:\n",
        "[Meta Learning Tutorial](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial16/Meta_Learning.ipynb)\n",
        "\n",
        "\n",
        "Prototypical Networks operate on the principle that data points within an embedding space cluster around a central prototype for each class as shown in Figure \\ref{protoFig}. Utilizing an encoder for non-linear mapping, the networks cluster features around a central prototype for each class within an embedding space. Classification becomes a nearest-neighbour problem, with the class of a query point determined by its proximity to these prototypes.\n",
        "ProtoBeam employs this concept of Prototypical Networks for beam classification where prototypes are created during training using a training set of a particular antenna.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/omarmshaal/ProtoBeam/main/images/protonet.png\" alt=\"Meta Learning Diagram\" width=\"700\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "Bq0NIo-AGa3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWNfVJ4sdNhk",
        "outputId": "5452b4cc-5a5f-4060-dca5-3954864a0883"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_118153/1403720320.py:16: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n",
            "Seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Device: cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from statistics import mean, stdev\n",
        "from copy import deepcopy\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    %pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Import tensorboard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial16\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9JbxfNod_6Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.utils.data as data\n",
        "import random\n",
        "\n",
        "class DeepBeamDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class for handling and preprocessing IQ data stored in Deepbeam HDF5 files.\n",
        "\n",
        "    Attributes:\n",
        "        data_file (str): Path to the HDF5 file containing the data.\n",
        "        label (str): The key for the label data in the HDF5 file.\n",
        "        classes (list): List of all class labels.\n",
        "        num_samples (int): Number of samples to be included in each data instance.\n",
        "        indices (list or None): Specific indices of samples to include. If None, all samples are included.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_file, label, classes, num_samples, indices=None, augment=True):\n",
        "        self.data_file = data_file\n",
        "        self.label = label\n",
        "        self.num_samples = num_samples\n",
        "        self.indices = indices  # Parameter to specify indices of samples to include\n",
        "        self.le = LabelEncoder()\n",
        "        self.le.fit(classes)\n",
        "        self.classes = classes\n",
        "        self.augment = augment  # Parameter to enable or disable data augmentation\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of data points in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        if self.indices is not None:\n",
        "            return len(self.indices)\n",
        "        with h5py.File(self.data_file, 'r') as f:\n",
        "            return len(f[self.label]) // self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves the data and label at the specified index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the data point to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (data, label) where data is a numpy array of the IQ samples and label is the corresponding class label.\n",
        "        \"\"\"\n",
        "        actual_idx = idx\n",
        "        with h5py.File(self.data_file, 'r') as f:\n",
        "            x = np.float32(f['iq'][self.num_samples*actual_idx:self.num_samples*(actual_idx+1)][:, :2048])\n",
        "            x = x[np.newaxis, :, :]\n",
        "\n",
        "            if self.augment:\n",
        "                scale_factor = random.uniform(0.5, 1.4)  # Generate a random scaling factor between 0 and 2\n",
        "                x *= scale_factor  # Scale the samples by this factor\n",
        "\n",
        "                # Apply phase rotation\n",
        "                rotation_angle = np.random.uniform(0, 2*np.pi)  # Random rotation angle\n",
        "                complex_signal = x[:, :, 0] + 1j * x[:, :, 1]  # Convert to complex\n",
        "                rotated_signal = complex_signal * np.exp(1j * rotation_angle)  # Rotate\n",
        "                x[:, :, 0], x[:, :, 1] = rotated_signal.real, rotated_signal.imag  # Update I and Q\n",
        "\n",
        "            # Normalization code remains unchanged\n",
        "            epsilon = 1e-9\n",
        "            x_min_i, x_max_i = x[:, :, 0].min(), x[:, :, 0].max()\n",
        "            x[:, :, 0] = 2 * ((x[:, :, 0] - x_min_i) / (x_max_i - x_min_i + epsilon)) - 1\n",
        "\n",
        "            x_min_q, x_max_q = x[:, :, 1].min(), x[:, :, 1].max()\n",
        "            x[:, :, 1] = 2 * ((x[:, :, 1] - x_min_q) / (x_max_q - x_min_q + epsilon)) - 1\n",
        "\n",
        "            y = np.int64([f[self.label][self.num_samples*actual_idx]])[0]\n",
        "\n",
        "        return x, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct6HB092eYTT"
      },
      "outputs": [],
      "source": [
        "def deepbeam_dataset_from_labels(data_file, label, num_samples, class_set, num_blocks, augment=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Creates a DeepBeamDataset with selected indices based on the specified class labels.\n",
        "\n",
        "    Args:\n",
        "        data_file (str): Path to the HDF5 file containing the data.\n",
        "        label (str): The key for the label data in the HDF5 file.\n",
        "        num_samples (int): Number of samples to be included in each data instance.\n",
        "        class_set (list): List of class labels to include in the dataset.\n",
        "        num_blocks (int): Number of blocks to skip between selected indices.\n",
        "        **kwargs: Additional arguments passed to the DeepBeamDataset constructor.\n",
        "\n",
        "    Returns:\n",
        "        DeepBeamDataset: An instance of the DeepBeamDataset class with the selected indices.\n",
        "    \"\"\"\n",
        "    frames_per_class = 150000  # Number of frames per class for the 24 beams dataset this number will change for 5 beams or AOA\n",
        "    num_classes_all = 24  # Total number of unique classes in the dataset\n",
        "    selected_indices = np.array([], dtype=int)  # Array to store selected indices\n",
        "\n",
        "    for class_label in class_set:\n",
        "        # Calculate the start and end frame index for each class in the first cycle\n",
        "        start_frame = (class_label * frames_per_class)\n",
        "        end_frame = (start_frame + frames_per_class)\n",
        "        start_idx = start_frame\n",
        "        end_idx = end_frame\n",
        "        gain_step = frames_per_class * num_classes_all\n",
        "\n",
        "        # Select indices for the specified class and concatenate to selected_indices\n",
        "        indices_for_class  = np.concatenate([\n",
        "            np.arange(start_idx, end_idx, num_blocks),\n",
        "            np.arange(start_idx + gain_step, end_idx + gain_step, num_blocks),\n",
        "            np.arange(start_idx + (2 * gain_step), end_idx + (2 * gain_step), num_blocks)\n",
        "        ])\n",
        "        selected_indices = np.concatenate([selected_indices, indices_for_class])\n",
        "\n",
        "    return DeepBeamDataset(data_file, label, torch.tensor(class_set), num_samples, selected_indices, augment=augment, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duQ4hQradNhr",
        "outputId": "527b162f-7832-4af6-fec5-8decdae3888b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([20,  5,  3, 12,  7,  2, 13, 17,  4,  1, 19, 22,  9, 18, 11,  8, 16, 10,\n",
            "         0, 14, 15, 23, 21,  6])\n",
            "tensor([18, 11,  8, 16, 10,  0, 14, 15, 23, 21,  6])\n",
            "tensor([20,  5,  3, 12,  7,  2, 13, 17,  4,  1, 19, 22,  9, 18, 11,  8, 16, 10,\n",
            "         0, 14, 15, 23, 21,  6])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)           # Set seed for reproducibility\n",
        "classes = torch.randperm(24)  # Returns random permutation of numbers 0 to 99\n",
        "train_classes, val_classes, test_classes = classes[:24], classes[13:24], classes[0:24]\n",
        "print(train_classes)\n",
        "print(val_classes)\n",
        "print(test_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofyTMZl7efrv",
        "outputId": "9366b6dc-25f5-4d71-b57e-ba439ae6db74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "86400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_118153/335162016.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return DeepBeamDataset(data_file, label, torch.tensor(class_set), num_samples, selected_indices, augment=augment, **kwargs)\n",
            "/tmp/ipykernel_118153/335162016.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return DeepBeamDataset(data_file, label, torch.tensor(class_set), num_samples, selected_indices, augment=augment, **kwargs)\n",
            "/tmp/ipykernel_118153/335162016.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return DeepBeamDataset(data_file, label, torch.tensor(class_set), num_samples, selected_indices, augment=augment, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Define the path to the HDF5 data file\n",
        "data_file =  \"/mnt/c/Users/ict_3/deepBeam_matlab/neu_ww72bk37k.h5\"\n",
        "label = 'tx_beam'  # Label key in the HDF5 file\n",
        "num_samples = 2048  # Number of samples per data point\n",
        "\n",
        "# Define the number of blocks to skip between selected indices for each dataset\n",
        "# Experiments shown that we do not need the whole dataset for training the model\n",
        "# Using part of the dataset for training is sufficient to achieve results similar using the full dataset\n",
        "train_n_blocks = 125\n",
        "val_n_blocks = 1400\n",
        "test_n_blocks = 450\n",
        "\n",
        "# Create training dataset\n",
        "train_set = deepbeam_dataset_from_labels(\n",
        "    data_file, label, num_samples, train_classes, train_n_blocks\n",
        ")\n",
        "\n",
        "# Create validation dataset\n",
        "val_set = deepbeam_dataset_from_labels(\n",
        "    data_file, label, num_samples, val_classes, val_n_blocks\n",
        ")\n",
        "\n",
        "# Create test dataset\n",
        "test_set = deepbeam_dataset_from_labels(\n",
        "    data_file, label, num_samples, test_classes, test_n_blocks\n",
        ")\n",
        "\n",
        "# Print the number of samples in the training dataset\n",
        "print(len(train_set))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3Y4aQNpeuBf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "class FewShotBatchSampler(object):\n",
        "    def __init__(self, total_frames, frames_per_class, N_way, K_shot, num_classes=24, class_set=[], include_query=False, shuffle=True, shuffle_once=False, blocks=50000):\n",
        "        \"\"\"\n",
        "        A batch sampler for few-shot learning, designed to sample batches of N-way K-shot examples.\n",
        "\n",
        "        Args:\n",
        "            total_frames (int): Total number of frames in the dataset.\n",
        "            frames_per_class (int): Number of frames per class before repeating.\n",
        "            N_way (int): Number of classes to sample per batch.\n",
        "            K_shot (int): Number of examples to sample per class in the batch.\n",
        "            num_classes (int, optional): Number of unique classes in a complete set before repeating. Defaults to 5.\n",
        "            class_set (list, optional): List of classes to include in the sampling. Defaults to an empty list.\n",
        "            include_query (bool, optional): If True, doubles the K_shot for support and query sets. Defaults to False.\n",
        "            shuffle (bool, optional): If True, examples and classes are newly shuffled in each iteration (for training). Defaults to True.\n",
        "            shuffle_once (bool, optional): If True, examples and classes are shuffled once at the beginning (for validation). Defaults to False.\n",
        "            blocks (int, optional): Number of blocks to skip between selected indices. Defaults to 50000.\n",
        "        \"\"\"\n",
        "        self.total_frames = total_frames\n",
        "        self.frames_per_class = frames_per_class\n",
        "        self.N_way = N_way\n",
        "        self.K_shot = K_shot * 2 if include_query else K_shot\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.shuffle_once = shuffle_once\n",
        "        self.include_query = include_query  # Ensure this attribute is correctly initialized\n",
        "        self.classes = class_set\n",
        "        self.num_blocks = blocks\n",
        "\n",
        "        # Validate N_way against the number of available classes\n",
        "        if N_way > len(self.classes):\n",
        "            raise ValueError(\"Number of sampled classes (N_way) cannot be larger than the number of available classes\")\n",
        "\n",
        "        self.indices_per_class = {}\n",
        "        self.batches_per_class = {}\n",
        "\n",
        "        frames_per_class = 150000\n",
        "        num_classes_all = 24\n",
        "        gain_step = frames_per_class * num_classes_all\n",
        "\n",
        "        # Generate indices for each class and store them\n",
        "        selected_indices = []\n",
        "        for c in self.classes:\n",
        "            start_frame = (c * frames_per_class)\n",
        "            end_frame = (start_frame + frames_per_class)\n",
        "            start_idx = start_frame\n",
        "            end_idx = end_frame\n",
        "\n",
        "            # Generate indices for this class and append to selected_indices\n",
        "            selected_indices = np.arange(start_idx, end_idx, self.num_blocks)\n",
        "            selected_indices = np.concatenate([\n",
        "                selected_indices,\n",
        "                np.arange(start_idx + gain_step, end_idx + gain_step, self.num_blocks),\n",
        "                np.arange(start_idx + (2 * gain_step), end_idx + (2 * gain_step), self.num_blocks)\n",
        "            ])\n",
        "            self.indices_per_class[c] = selected_indices\n",
        "            self.batches_per_class[c] = self.indices_per_class[c].shape[0] // self.K_shot\n",
        "            selected_indices = []\n",
        "\n",
        "        self.batch_size = self.N_way * self.K_shot\n",
        "        self.iterations = sum(self.batches_per_class.values()) // self.N_way\n",
        "        self.class_list = [c for c in self.classes for _ in range(self.batches_per_class[c])]\n",
        "\n",
        "        # Shuffle data once if shuffle_once is set\n",
        "        if shuffle_once:\n",
        "            self.shuffle_data()\n",
        "\n",
        "    def shuffle_data(self):\n",
        "        \"\"\"Shuffles the indices per class and the class list.\"\"\"\n",
        "        for c in self.classes:\n",
        "            perm = torch.randperm(self.indices_per_class[c].shape[0])\n",
        "            self.indices_per_class[c] = self.indices_per_class[c][perm]\n",
        "        random.shuffle(self.class_list)\n",
        "        # Shuffle the class list from which we sample. Note that this way of shuffling\n",
        "        # does not prevent to choose the same class twice in a batch. However, for\n",
        "        # training and validation, this is not a problem.\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yields batches of indices for the few-shot learning task.\"\"\"\n",
        "        if self.shuffle:\n",
        "            self.shuffle_data()\n",
        "\n",
        "        start_index = defaultdict(int)\n",
        "        for it in range(self.iterations):\n",
        "            class_batch = self.class_list[it * self.N_way : (it + 1) * self.N_way]  # Select N classes for the batch\n",
        "            index_batch = []\n",
        "            for c in class_batch:  # For each class, select the next K examples and add them to the batch\n",
        "                index_batch.extend(self.indices_per_class[c][start_index[c] : start_index[c] + self.K_shot])\n",
        "                start_index[c] += self.K_shot\n",
        "\n",
        "            if self.include_query:  # If we return support + query set, sort them so that they are easy to split\n",
        "                index_batch = index_batch[::2] + index_batch[1::2]\n",
        "\n",
        "            yield index_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of iterations (batches) per epoch.\"\"\"\n",
        "        return self.iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbHtjux3e4Hj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class CustomSampler(object):\n",
        "    def __init__(self, indices, shuffle=False):\n",
        "        \"\"\"\n",
        "        Custom sampler for iterating over a list of indices.\n",
        "\n",
        "        Args:\n",
        "            indices (array-like): List or array of indices to sample from.\n",
        "            shuffle (bool, optional): If True, shuffles the indices before sampling. Defaults to False.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Assuming 'indices' is a numpy array; convert it to a list to ensure compatibility\n",
        "        self.indices = indices.tolist()\n",
        "        if shuffle:\n",
        "            random.shuffle(self.indices)\n",
        "\n",
        "        # Debugging print to check the type of indices\n",
        "        print(type(self.indices))\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Yields indices one by one.\n",
        "\n",
        "        Yields:\n",
        "            int: The next index from the list of indices.\n",
        "        \"\"\"\n",
        "        for index in self.indices:\n",
        "            yield index\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of indices.\n",
        "\n",
        "        Returns:\n",
        "            int: The length of the indices list.\n",
        "        \"\"\"\n",
        "        return len(self.indices)\n",
        "\n",
        "# Example usage:\n",
        "# custom_sampler = CustomSampler(indices=test_indi, shuffle=False)\n",
        "# data_loader = data.DataLoader(test_set, sampler=custom_sampler, batch_size=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMtQ5OI0e__u",
        "outputId": "8eb3fbc0-024c-4a60-af0b-58e8b945d3eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1350\n",
            "54\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoader for training set\n",
        "train_data_loader = data.DataLoader(\n",
        "    train_set,\n",
        "    batch_sampler=FewShotBatchSampler(\n",
        "        total_frames=10800000,        # Total number of frames in the dataset\n",
        "        frames_per_class=150000,      # Number of frames per class before repeating\n",
        "        N_way=8,                      # Number of classes to sample per batch\n",
        "        K_shot=4,                     # Number of examples to sample per class in the batch\n",
        "        num_classes=len(train_classes),  # Total number of unique classes in the training set\n",
        "        class_set=train_classes.numpy(), # Classes to include in sampling, adjusted based on dataset\n",
        "        include_query=True,           # If True, doubles the K_shot for support and query sets\n",
        "        shuffle=True,                 # Shuffle examples and classes in each iteration (for training)\n",
        "        blocks=train_n_blocks         # Number of blocks to skip between selected indices\n",
        "    ),\n",
        "    num_workers=8  # Number of worker processes to use for data loading\n",
        ")\n",
        "\n",
        "# Create DataLoader for validation set\n",
        "val_data_loader = data.DataLoader(\n",
        "    val_set,\n",
        "    batch_sampler=FewShotBatchSampler(\n",
        "        total_frames=10800000,        # Total number of frames in the dataset\n",
        "        frames_per_class=150000,      # Number of frames per class before repeating\n",
        "        N_way=11,                     # Number of classes to sample per batch\n",
        "        K_shot=3,                     # Number of examples to sample per class in the batch\n",
        "        num_classes=len(val_classes), # Total number of unique classes in the validation set\n",
        "        class_set=val_classes.numpy(),# Classes to include in sampling, adjusted based on dataset\n",
        "        include_query=True,           # If True, doubles the K_shot for support and query sets\n",
        "        shuffle=True,                 # Shuffle examples and classes in each iteration (for training)\n",
        "        blocks=val_n_blocks           # Number of blocks to skip between selected indices\n",
        "    ),\n",
        "    num_workers=8  # Number of worker processes to use for data loading\n",
        ")\n",
        "\n",
        "# Print the number of batches in the training DataLoader\n",
        "print(len(train_data_loader))\n",
        "\n",
        "# Print the number of batches in the validation DataLoader\n",
        "print(len(val_data_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETjaI9tSfGIM"
      },
      "outputs": [],
      "source": [
        "def split_batch(IQ, targets):\n",
        "    \"\"\"\n",
        "    Splits a batch of IQ samples and targets into support and query sets.\n",
        "\n",
        "    Args:\n",
        "        IQ (Tensor): Batch of IQ samples, assumed to be of shape (2 * batch_size, ...).\n",
        "        targets (Tensor): Batch of targets, assumed to be of shape (2 * batch_size, ...).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
        "            - support_IQ (Tensor): Support set IQ samples of shape (batch_size, ...).\n",
        "            - query_IQ (Tensor): Query set IQ samples of shape (batch_size, ...).\n",
        "            - support_targets (Tensor): Support set targets of shape (batch_size, ...).\n",
        "            - query_targets (Tensor): Query set targets of shape (batch_size, ...).\n",
        "    \"\"\"\n",
        "    # Split the IQ samples tensor into support and query sets along the first dimension\n",
        "    support_IQ, query_IQ = IQ.chunk(2, dim=0)\n",
        "\n",
        "    # Split the targets tensor into support and query sets along the first dimension\n",
        "    support_targets, query_targets = targets.chunk(2, dim=0)\n",
        "\n",
        "    return support_IQ, query_IQ, support_targets, query_targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIm7TCyjfMwU"
      },
      "outputs": [],
      "source": [
        "def get_convnet(output_size):\n",
        "    \"\"\"\n",
        "    Creates a DenseNet-based convolutional neural network for a specified output size.\n",
        "\n",
        "    Args:\n",
        "        output_size (int): The number of output classes for the classification task.\n",
        "\n",
        "    Returns:\n",
        "        torchvision.models.DenseNet: A DenseNet model configured with the specified output size.\n",
        "    \"\"\"\n",
        "    # Create a DenseNet model with the specified configuration and output size\n",
        "    convnet = torchvision.models.DenseNet(\n",
        "        growth_rate=32,          # Number of filters to add each layer (growth rate)\n",
        "        block_config=(5, 5, 5),  # Number of layers in each dense block\n",
        "        bn_size=1,               # Multiplicative factor for bottleneck layers\n",
        "        num_init_features=64,    # Number of filters in the initial convolution layer\n",
        "        num_classes=output_size  # Output dimensionality\n",
        "    )\n",
        "    return convnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx5u8iGffrLs"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class ProtoNet(pl.LightningModule):\n",
        "    def __init__(self, proto_dim, lr):\n",
        "        \"\"\"\n",
        "        Initializes the ProtoNet model.\n",
        "\n",
        "        Args:\n",
        "            proto_dim (int): Dimensionality of the prototype feature space.\n",
        "            lr (float): Learning rate for the Adam optimizer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = get_convnet(output_size=self.hparams.proto_dim)\n",
        "        self.model.features.conv0 = nn.Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1), padding=(3, 3), bias=False)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures the optimizers and learning rate scheduler.\n",
        "\n",
        "        Returns:\n",
        "            list: List of optimizers.\n",
        "            list: List of learning rate schedulers.\n",
        "        \"\"\"\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 80], gamma=0.1)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_prototypes(features, targets):\n",
        "        \"\"\"\n",
        "        Calculates class prototypes from features and targets.\n",
        "\n",
        "        Args:\n",
        "            features (Tensor): Tensor of shape [N, proto_dim] containing feature vectors.\n",
        "            targets (Tensor): Tensor of shape [N] containing class labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Prototypes for each class.\n",
        "            Tensor: Unique classes.\n",
        "        \"\"\"\n",
        "        classes, _ = torch.unique(targets).sort()  # Determine which classes we have\n",
        "        prototypes = []\n",
        "        for c in classes:\n",
        "            p = features[torch.where(targets == c)[0]].mean(dim=0)  # Average class feature vectors\n",
        "            p = p / p.norm(dim=0, keepdim=True)  # Normalize the prototype\n",
        "            prototypes.append(p)\n",
        "        prototypes = torch.stack(prototypes, dim=0)\n",
        "        return prototypes, classes\n",
        "\n",
        "    def classify_feats(self, prototypes, classes, feats, targets):\n",
        "        \"\"\"\n",
        "        Classifies new examples with prototypes and returns classification error.\n",
        "\n",
        "        Args:\n",
        "            prototypes (Tensor): Tensor containing class prototypes.\n",
        "            classes (Tensor): Tensor containing unique class labels.\n",
        "            feats (Tensor): Tensor containing feature vectors to classify.\n",
        "            targets (Tensor): Tensor containing true class labels for the features.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Log-softmax predictions.\n",
        "            Tensor: True labels.\n",
        "            Tensor: Accuracy of the predictions.\n",
        "        \"\"\"\n",
        "        dist = torch.pow(prototypes[None, :] - feats[:, None], 2).sum(dim=2)  # Squared Euclidean distance\n",
        "        preds = F.log_softmax(-dist, dim=1)\n",
        "        labels = (classes[None, :] == targets[:, None]).long().argmax(dim=-1)\n",
        "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
        "        return preds, labels, acc\n",
        "\n",
        "\n",
        "    def calculate_loss(self, batch, mode):\n",
        "        \"\"\"\n",
        "        Calculates training loss for a given support and query set.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): Tuple containing images and targets.\n",
        "            mode (str): Mode for logging, e.g., \"train\" or \"val\".\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Calculated loss.\n",
        "        \"\"\"\n",
        "        iqs, targets = batch\n",
        "        features = self.model(iqs)  # Encode all IQ samples of support and query set\n",
        "        support_feats, query_feats, support_targets, query_targets = split_batch(features, targets)\n",
        "        prototypes, classes = ProtoNet.calculate_prototypes(support_feats, support_targets)\n",
        "        preds, labels, acc = self.classify_feats(prototypes, classes, query_feats, query_targets)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        self.log(f\"{mode}_loss\", loss)\n",
        "        self.log(f\"{mode}_acc\", acc)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Performs a single training step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): Tuple containing images and targets.\n",
        "            batch_idx (int): Index of the current batch.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Calculated loss for the batch.\n",
        "        \"\"\"\n",
        "        return self.calculate_loss(batch, mode=\"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Performs a single validation step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): Tuple containing images and targets.\n",
        "            batch_idx (int): Index of the current batch.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        _ = self.calculate_loss(batch, mode=\"val\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sED5EBKgepx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
        "\n",
        "def train_model(model_class, train_loader, val_loader, **kwargs):\n",
        "    \"\"\"\n",
        "    Trains a PyTorch Lightning model with the specified training and validation data loaders.\n",
        "\n",
        "    Args:\n",
        "        model_class (pl.LightningModule): The class of the model to be trained.\n",
        "        train_loader (DataLoader): DataLoader for the training data.\n",
        "        val_loader (DataLoader): DataLoader for the validation data.\n",
        "        **kwargs: Additional keyword arguments for the model class initialization.\n",
        "\n",
        "    Returns:\n",
        "        pl.LightningModule: The trained model.\n",
        "    \"\"\"\n",
        "    trainer = pl.Trainer(\n",
        "        default_root_dir=os.path.join(CHECKPOINT_PATH, model_class.__name__),\n",
        "        accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "        devices=1,\n",
        "        max_epochs=110,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
        "            LearningRateMonitor(\"epoch\")\n",
        "        ],\n",
        "        enable_progress_bar=True\n",
        "    )\n",
        "    trainer.logger._default_hp_metric = None\n",
        "\n",
        "    # Check whether a pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, model_class.__name__ + \".ckpt\")\n",
        "    print(pretrained_filename)\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
        "        # Automatically loads the model with the saved hyperparameters\n",
        "        model = model_class.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42)  # Ensure reproducibility\n",
        "        model = model_class(**kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        model = model_class.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)  # Load best checkpoint after training\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4SGqnwDgj2t",
        "outputId": "1c43de78-065c-4fcf-e7b8-fc7ad2430caf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Seed set to 42\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../saved_models/tutorial16/ProtoNet.ckpt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name  | Type     | Params\n",
            "-----------------------------------\n",
            "0 | model | DenseNet | 326 K \n",
            "-----------------------------------\n",
            "326 K     Trainable params\n",
            "0         Non-trainable params\n",
            "326 K     Total params\n",
            "1.308     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11:  41%|████▏     | 557/1350 [01:52<02:40,  4.96it/s, v_num=170]    "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/root01/deepbeam/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
          ]
        }
      ],
      "source": [
        "# Train the ProtoNet model with specified parameters\n",
        "protonet_model = train_model(\n",
        "    ProtoNet,          # Model class to be trained\n",
        "    proto_dim=128,     # Dimensionality of prototype feature space\n",
        "    lr=1e-3,           # Learning rate for the optimizer\n",
        "    train_loader=train_data_loader,  # DataLoader for the training data\n",
        "    val_loader=val_data_loader       # DataLoader for the validation data\n",
        ")\n",
        "# Note: You can adjust the learning rate (lr) as needed, e.g., lr=2e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG_37QYndNh4"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /mnt/c/Users/ict_3/saved_models/tutorial16/ProtoNet/lightning_logs/version_160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_llQUFig4xi"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_proto_net(model, dataset, data_feats=None, k_shot=4):\n",
        "    \"\"\"\n",
        "    Tests a pretrained ProtoNet model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (ProtoNet): Pretrained ProtoNet model.\n",
        "        dataset (ImageDataset): The dataset on which the test should be performed.\n",
        "        data_feats (tuple, optional): The encoded features of all images in the dataset.\n",
        "                                      If None, they will be newly calculated and returned for later usage.\n",
        "        k_shot (int, optional): Number of examples per class in the support set. Defaults to 4.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Mean and standard deviation of accuracies, encoded features and targets, and prototypes.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    num_classes = len(test_classes)\n",
        "    exmps_per_class = int(len(dataset) / num_classes)  # Assumes uniform example distribution\n",
        "\n",
        "    if data_feats is None:\n",
        "        # Dataset preparation\n",
        "        custom_sampler = CustomSampler(indices=dataset.indices, shuffle=True)\n",
        "        dataloader = data.DataLoader(dataset, sampler=custom_sampler, batch_size=32, num_workers=6)\n",
        "\n",
        "        iq_features = []\n",
        "        iq_targets = []\n",
        "        for iqs, targets in tqdm(dataloader, \"Extracting IQ samples features\", leave=False):\n",
        "            iqs = iqs.to(device)\n",
        "            feats = model.model(iqs)\n",
        "            iq_features.append(feats.detach().cpu())\n",
        "            iq_targets.append(targets)\n",
        "        iq_features = torch.cat(iq_features, dim=0)\n",
        "        iq_targets = torch.cat(iq_targets, dim=0)\n",
        "\n",
        "        # Sort by classes for easier processing later\n",
        "        iq_targets, sort_idx = iq_targets.sort()\n",
        "        iq_targets = iq_targets.reshape(num_classes, exmps_per_class).transpose(0, 1)\n",
        "        iq_features = iq_features[sort_idx].reshape(num_classes, exmps_per_class, -1).transpose(0, 1)\n",
        "    else:\n",
        "        iq_features, iq_targets = data_feats\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    # Evaluate the model using k-shot batches\n",
        "    for k_idx in tqdm(range(0, iq_features.shape[0], k_shot), \"Evaluating prototype classification\", leave=False):\n",
        "        # Select support set and calculate prototypes\n",
        "        k_iq_feats, k_targets = iq_features[k_idx:k_idx+k_shot].flatten(0, 1), iq_targets[k_idx:k_idx+k_shot].flatten(0, 1)\n",
        "        prototypes, proto_classes = model.calculate_prototypes(k_iq_feats, k_targets)\n",
        "\n",
        "        # Evaluate accuracy on the rest of the dataset\n",
        "        batch_acc = 0\n",
        "        for e_idx in range(0, iq_features.shape[0], k_shot):\n",
        "            if k_idx == e_idx:  # Do not evaluate on the support set examples\n",
        "                continue\n",
        "            e_iq_feats, e_targets = iq_features[e_idx:e_idx+k_shot].flatten(0, 1), iq_targets[e_idx:e_idx+k_shot].flatten(0, 1)\n",
        "            _, _, acc = model.classify_feats(prototypes, proto_classes, e_iq_feats, e_targets)\n",
        "            batch_acc += acc.item()\n",
        "\n",
        "        batch_acc /= (len(range(0, iq_features.shape[0], k_shot)) - 1)\n",
        "        accuracies.append(batch_acc)\n",
        "\n",
        "    return (mean(accuracies), stdev(accuracies)), (iq_features, iq_targets), prototypes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtPMcRsihCrE",
        "outputId": "06547853-6e92-44f6-af64-9751a5e8dea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=2: 80.16% (+-1.35%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=8: 82.65% (+-0.63%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=16: 83.12% (+-0.43%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=32: 83.40% (+-0.44%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=64: 83.58% (+-0.25%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=128: 83.70% (+-0.21%)\n"
          ]
        }
      ],
      "source": [
        "# Dictionary to store accuracies for different k-shot values\n",
        "protonet_accuracies = dict()\n",
        "\n",
        "# Variable to store encoded features for later usage\n",
        "data_feats = None\n",
        "\n",
        "# Iterate over different k-shot values and test the ProtoNet model\n",
        "for k in [2, 8, 16, 32, 64, 128]:\n",
        "    # Test the model with the current k-shot value\n",
        "    protonet_accuracies[k], data_feats, Proto = test_proto_net(protonet_model, test_set, data_feats=data_feats, k_shot=k)\n",
        "\n",
        "    # Print the accuracy for the current k-shot value\n",
        "    print(f\"Accuracy for k={k}: {100.0 * protonet_accuracies[k][0]:4.2f}% (+-{100 * protonet_accuracies[k][1]:4.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Model on a Different Dataset for Domain Adaptation\n",
        "\n",
        "This part is dedicated to testing the model on a different dataset other than the training dataset to evaluate its performance in the context of domain adaptation. Domain adaptation is crucial for ensuring that the model can generalize well to new, unseen data from different distributions.\n"
      ],
      "metadata": {
        "id": "Gn9KhywGGh7z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT0HnWnrdNh6",
        "outputId": "ee73ced2-3d75-4864-a166-1b0e079543e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54000\n",
            "2250.0\n"
          ]
        }
      ],
      "source": [
        "data_file2 =  \"/mnt/c/Users/ict_3/deepBeam_matlab/neu_ww72bk483.h5\"\n",
        "data_file3 = \"/mnt/c/Users/ict_3/Documents/Downloads/neu_ww72bk46j.h5\"\n",
        "data_file4 = \"/mnt/c/Users/ict_3/Documents/Downloads/neu_ww72bk458.h5\"\n",
        "\n",
        "test_classes2= list(range(0,24))\n",
        "\n",
        "\n",
        "test_set2 = deepbeam_dataset_from_labels(\n",
        "    data_file2, label, num_samples, test_classes2,200,augment=False)\n",
        "\n",
        "test_set3 = deepbeam_dataset_from_labels(\n",
        "    data_file3, label, num_samples, test_classes2,200,augment=False)\n",
        "\n",
        "test_set4 = deepbeam_dataset_from_labels(\n",
        "    data_file4, label, num_samples, test_classes2,200,augment=False)\n",
        "print(len(test_set2))\n",
        "num_classes=len(test_classes2)\n",
        "#exmps_per_class = dataset.targets.shape[0]//num_classes  # We assume uniform example distribution here\n",
        "exmps_per_class= len(test_set2)/num_classes\n",
        "print(exmps_per_class)\n",
        "# label_counts = defaultdict(int)\n",
        "# i=0\n",
        "# for i in test_set2.indices:\n",
        "#     x,y = test_set2[i]\n",
        "#     label_counts[y]+=1\n",
        "# label_counts = dict(label_counts)\n",
        "# print(label_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdK4QML8dNh7"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_proto_net2(model, dataset, data_feats=None, k_shot=4,pr=0):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "        model - Pretrained ProtoNet model\n",
        "        dataset - The dataset on which the test should be performed.\n",
        "                  Should be instance of ImageDataset\n",
        "        data_feats - The encoded features of all images in the dataset.\n",
        "                     If None, they will be newly calculated, and returned\n",
        "                     for later usage.\n",
        "        k_shot - Number of examples per class in the support set.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    #num_classes = dataset.targets.unique().shape[0]\n",
        "    num_classes=len(test_classes2)\n",
        "    #exmps_per_class = dataset.targets.shape[0]//num_classes  # We assume uniform example distribution here\n",
        "    exmps_per_class= int(len(dataset)/num_classes )   # The encoder network remains unchanged across k-shot settings. Hence, we only need\n",
        "    # to extract the features for all images once.\n",
        "    if data_feats is None:\n",
        "        # Dataset preparation\n",
        "        custom_sampler = CustomSampler(indices=dataset.indices,shuffle=True)\n",
        "\n",
        "        dataloader = data.DataLoader(dataset, sampler=custom_sampler, batch_size=64,num_workers=6)\n",
        "        #dataloader = data.DataLoader(dataset.indicies, batch_size=128, num_workers=4, shuffle=False, drop_last=False)\n",
        "\n",
        "        iq_features = []\n",
        "        iq_targets = []\n",
        "        for iqs, targets in tqdm(dataloader, \"Extracting IQ samples features\", leave=False):\n",
        "            iqs = iqs.to(device)\n",
        "            feats = model.model(iqs)\n",
        "            iq_features.append(feats.detach().cpu())\n",
        "            iq_targets.append(targets)\n",
        "        iq_features = torch.cat(iq_features, dim=0)\n",
        "        iq_targets = torch.cat(iq_targets, dim=0)\n",
        "        # Sort by classes, so that we obtain tensors of shape [num_classes, exmps_per_class, ...]\n",
        "        # Makes it easier to process later\n",
        "        iq_targets, sort_idx = iq_targets.sort()\n",
        "        iq_targets = iq_targets.reshape(num_classes, exmps_per_class).transpose(0, 1)\n",
        "        iq_features = iq_features[sort_idx].reshape(num_classes, exmps_per_class, -1).transpose(0, 1)\n",
        "    else:\n",
        "        iq_features, iq_targets = data_feats\n",
        "\n",
        "    # We iterate through the full dataset in two manners. First, to select the k-shot batch.\n",
        "    # Second, the evaluate the model on all other examples\n",
        "    accuracies = []\n",
        "    for k_idx in tqdm(range(0, iq_features.shape[0], k_shot), \"Evaluating prototype classification\", leave=False):\n",
        "        # Select support set and calculate prototypes\n",
        "        k_iq_feats, k_targets = iq_features[k_idx:k_idx+k_shot].flatten(0,1), iq_targets[k_idx:k_idx+k_shot].flatten(0,1)\n",
        "        prototypes, proto_classes = model.calculate_prototypes(k_iq_feats, k_targets)\n",
        "        #prototypes=(pr/5+prototypes)\n",
        "        # Evaluate accuracy on the rest of the dataset\n",
        "        batch_acc = 0\n",
        "        for e_idx in range(0, iq_features.shape[0], k_shot):\n",
        "            if k_idx == e_idx:  # Do not evaluate on the support set examples\n",
        "                continue\n",
        "            e_iq_feats, e_targets = iq_features[e_idx:e_idx+k_shot].flatten(0,1), iq_targets[e_idx:e_idx+k_shot].flatten(0,1)\n",
        "\n",
        "            _, _, acc = model.classify_feats(prototypes, proto_classes, e_iq_feats, e_targets)\n",
        "\n",
        "            batch_acc += acc.item()\n",
        "        batch_acc /= (len(list(range(0, iq_features.shape[0], k_shot)))-1)\n",
        "        accuracies.append(batch_acc)\n",
        "\n",
        "    return (mean(accuracies), stdev(accuracies)), (iq_features, iq_targets)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nppf0OytdNh7",
        "outputId": "abce93c1-72ca-4b9d-cbc9-b8cd6f239276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=2: 57.02% (+-3.04%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=8: 64.45% (+-0.78%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=16: 65.36% (+-0.51%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=32: 65.79% (+-0.38%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=64: 66.10% (+-0.33%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for k=128: 66.36% (+-0.21%)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "protonet_DP2_accuracies = dict()\n",
        "#data_feats = None\n",
        "data_feats2=None\n",
        "for k in [2,8,16,32,64,128]:\n",
        "    protonet_DP2_accuracies[k], data_feats2 = test_proto_net2(protonet_model, test_set2, data_feats=data_feats2, k_shot=k,pr=0)\n",
        "    print(f\"Accuracy for k={k}: {100.0*protonet_DP2_accuracies[k][0]:4.2f}% (+-{100*protonet_DP2_accuracies[k][1]:4.2f}%)\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deepbeam",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}